{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy, csv, time, re, urllib.error, os\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook was running on a remote machine. We created logs to be able to check the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "today = date.today().isoformat()\n",
    "logging.basicConfig(filename='progress.log', format='%(asctime)s %(message)s', datefmt='%Y/%m/%d %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get your [own keys and secrets](https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'xxx'\n",
    "consumer_secret = 'xxx'\n",
    "access_token = 'xxx'\n",
    "access_token_secret = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create Tweepy handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Search API has a 15 minute window with limited amount of calls. We needed to sleep the script once the limit was reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def handle_errors(cursor):\n",
    "    # sleep the script for 16 minutes\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "        except tweepy.TweepError as e:\n",
    "            logging.warning(e)\n",
    "            time.sleep(16 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of users who liked a post are not a part of the Twitter API, only the number of likes. We therefore used a scraper [based on this StackOverflow issue](http://stackoverflow.com/questions/28982850/twitter-api-getting-list-of-users-who-favorited-a-status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_ids_of_post_likes(post_id):\n",
    "    \n",
    "    try:\n",
    "        json_data = urlopen('https://twitter.com/i/activity/favorited_popup?id=' + str(post_id)).read()\n",
    "        json_data = str(json_data)\n",
    "\n",
    "        found_names = re.findall(r'data-screen-name=\\\\\\\\\\\"[a-zA-Z0-9_]+', json_data)\n",
    "        unique_names = list(set([match.replace('data-screen-name=\\\\\\\\\\\"',\"\") for match in found_names]))\n",
    "\n",
    "        return unique_names\n",
    "    except urllib.error.HTTPError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API reference: https://dev.twitter.com/overview/api/tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweet_details(tweet, df, row):\n",
    "    # collect tweet details\n",
    "    \n",
    "    global users\n",
    "    global statuses \n",
    "    global rts \n",
    "    global columns\n",
    "    \n",
    "    # if tweet is a reply, get outta here\n",
    "    if tweet.in_reply_to_status_id_str != None:\n",
    "        return\n",
    "    \n",
    "    # who favorited? = scraper above\n",
    "    if tweet.favorite_count > 0:\n",
    "        favorite_users = get_user_ids_of_post_likes(tweet.id)\n",
    "        favorite_users.remove(tweet.user.screen_name)\n",
    "        favorite_users = \";\".join(favorite_users)\n",
    "    else:\n",
    "        favorite_users = []\n",
    "\n",
    "    users.append(tweet.user.screen_name)\n",
    "    statuses.append(tweet.id_str)\n",
    "    \n",
    "    try:\n",
    "        original_tweet_id = tweet.retweeted_status.id_str\n",
    "        original_tweet_user = tweet.retweeted_status.user.screen_name\n",
    "    except:\n",
    "        original_tweet_id = \"\"\n",
    "        original_tweet_user = \"\"\n",
    "        \n",
    "    rts.append(original_tweet_id) # get original statuses from retweets\n",
    "\n",
    "    values = [tweet.id_str,original_tweet_id,original_tweet_user,tweet.created_at,tweet.user.id_str,tweet.user.screen_name,\n",
    "              tweet.user.followers_count,tweet.user.friends_count,tweet.user.description,\n",
    "              tweet.user.statuses_count,tweet.text,tweet.favorite_count,favorite_users,\n",
    "              tweet.retweet_count,str(tweet.entities)]   \n",
    "\n",
    "    for c,v in zip(columns, values):\n",
    "        df.loc[row,c] = v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET all recent tweets on keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensure all parameters are properly URL encoded.\n",
    "- Limit your searches to 10 keywords and operators.\n",
    "- The Search API is not complete index of all Tweets, but instead an index of recent Tweets. The index includes between 6-9 days of Tweets.\n",
    "\n",
    "[query operators](https://dev.twitter.com/rest/public/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chose your keywords\n",
    "keywords = [\"klimaat\", \"co2\", \"climate\", \"IPCC\", \"windmolen\", \"zeespiegel\", \"turbine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET all the tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = []\n",
    "statuses = []\n",
    "rts = []\n",
    "\n",
    "columns = [\"tweet_id\",\"original_tweet_id\",\"original_tweet_user\",\"date_time\",\"user_id\",\"username\",\"followers\",\n",
    "           \"friends\",\"user_description\",\"statuses\",\"text\",\n",
    "           \"nr_likes\",\"users_likes\",\"nr_RT\",\"entities\"]\n",
    "\n",
    "df1 = pd.DataFrame(columns=columns)\n",
    "\n",
    "for n,tweet in enumerate(handle_errors(tweepy.Cursor(api.search,\n",
    "                       q=\" OR \".join(keywords),\n",
    "                       rpp=100,\n",
    "                       result_type=\"recent\",\n",
    "                       include_entities=True,\n",
    "                       lang=\"nl\").items())): #you can change the language here\n",
    "\n",
    "    tweet_details(tweet, df1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several types of tweets you will get while from the Search API: **tweets, replies** and **re-tweets**<br>**Replies** that contain the keywords were often off-topic. We excluded them from the analysis.<br>**Re-tweets** were a *reaction*. But we not used them only to draw connections between the users, but also to get the original tweets (if not already in the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get deduplicated list of statusID to match against\n",
    "statuses = list(set(statuses))\n",
    "# get a list of original Tweet IDs of re-tweets to use in collecting the re-tweet parents\n",
    "rts = list(set(rts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=columns)\n",
    "\n",
    "for n,r in enumerate(rts):\n",
    "    # get all original Tweets to the collected re-tweets\n",
    "    if r not in statuses:\n",
    "        logging.warning(\"%s : %s\" % (today,n))\n",
    "        \n",
    "        try:\n",
    "            tweet = api.get_status(r)\n",
    "            tweet_details(tweet, df2, n)\n",
    "\n",
    "        # wait for 16 minutes after reaching the rate limit\n",
    "        except tweepy.RateLimitError as e:\n",
    "            logging.warning(\"%s not processed because of %s\" % (r, e.api_code)) \n",
    "            time.sleep(16 * 60)\n",
    "\n",
    "        # some tweets are protected from retrieving so they were not collected\n",
    "        except tweepy.TweepError as e:\n",
    "            logging.warning(\"%s not processed because of %s\" % (r, e.api_code)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API does not offer anything on **replies** to a Tweet (number, usernames).<br> It does however mention if a particular Tweet was a reply to another tweet.<br> We therefore searched for ALL the replies to ALL the users in the data set. Then we filtered them to keep only the ones that reply to Twitter statuses in the data set. This part took the longest (about a day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_columns = [\"tweet_id\",\"date_time\",\"user_id\",\"username\",\"response_to_user_id\",\"response_to_username\",\"response_to_status_id\"]\n",
    "\n",
    "df3 = pd.DataFrame(columns=r_columns)\n",
    "\n",
    "count = 0\n",
    "row = 0\n",
    "\n",
    "for us in np.array_split(users,len(users)/10): # http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    # collect replies to Tweets\n",
    "    count +=10\n",
    "    logging.warning(count)\n",
    "    searchterm = \" OR \".join([\"to:\"+x for x in us])\n",
    "    \n",
    "    for tweet in handle_errors(tweepy.Cursor(api.search,\n",
    "                               q=searchterm,\n",
    "                               rpp=100,\n",
    "                               result_type=\"recent\",\n",
    "                               include_entities=True,\n",
    "                               lang=\"nl\").items()): #you can change the language here\n",
    "\n",
    "        if tweet.in_reply_to_status_id_str in statuses:\n",
    "            row += 1\n",
    "            r_values = [tweet.id_str,tweet.created_at,tweet.user.id_str,tweet.user.screen_name,\n",
    "                 tweet.in_reply_to_user_id_str,tweet.in_reply_to_screen_name, tweet.in_reply_to_status_id_str]\n",
    "\n",
    "            for c,v in zip(r_columns, r_values):\n",
    "                df3.loc[row,c] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged original re-tweeted tweets with the main data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df1.append(df2).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created new columns from the *replies* data to add to the main data set: *number of responses* and *users that responded*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count replies\n",
    "nr_responses = df3.groupby(\"response_to_status_id\").count()[\"tweet_id\"]\n",
    "\n",
    "# get usernames\n",
    "users_responses = df3.groupby(\"response_to_status_id\")[\"username\"].apply(lambda x: \";\".join(x.tolist()))\n",
    "\n",
    "# join the datasets\n",
    "calc = pd.concat([nr_responses, users_responses], axis=1)\n",
    "calc.columns = [\"nr_responses\", \"users_responses\"]\n",
    "calc = calc.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "dataset = pd.merge(left=data, right=calc, left_on = \"tweet_id\", right_on=\"response_to_status_id\", how=\"left\")\n",
    "dataset = dataset.drop(\"response_to_status_id\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKUP RAW DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(today):\n",
    "    os.makedirs(today)\n",
    "    \n",
    "df1.to_csv(\"%s/%s-sleutelwoorden_matches.csv\" % (today,today))\n",
    "df2.to_csv(\"%s/%s-retweets.csv\" % (today,today))\n",
    "df3.to_csv(\"%s/%s-replies.csv\" % (today,today))\n",
    "dataset.to_csv(\"%s/%s-klimaat_tweets.csv\" % (today,today))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
